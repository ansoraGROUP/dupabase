/**
 * E2E Import Feature Test Suite
 *
 * Tests the database import feature end-to-end:
 * 1. Create platform user + project
 * 2. Create test tables with data
 * 3. Export with pg_dump
 * 4. Create a second project
 * 5. Import the dump into the second project
 * 6. Verify data was imported correctly via supabase-js
 * 7. Test SQL format import
 * 8. Test error handling
 */

import { createClient } from '@supabase/supabase-js';
import { execSync } from 'child_process';
import { writeFileSync, unlinkSync, existsSync, mkdirSync } from 'fs';
import path from 'path';

const API = 'http://localhost:3333';
const PG_HOST = 'localhost';
const PG_PORT = 15432;
const PG_USER = 'stech';
const PG_PASS = 'S0cr%40t123';

let passed = 0;
let failed = 0;

function assert(condition, msg) {
  if (condition) {
    passed++;
    console.log(`  \x1b[32m✓\x1b[0m ${msg}`);
  } else {
    failed++;
    console.log(`  \x1b[31m✗\x1b[0m ${msg}`);
  }
}

async function api(method, path, body, token) {
  const h = { 'Content-Type': 'application/json' };
  if (token) h['Authorization'] = `Bearer ${token}`;
  const opts = { method, headers: h };
  if (body && method !== 'GET') opts.body = JSON.stringify(body);
  const res = await fetch(`${API}${path}`, opts);
  return { status: res.status, data: await res.json().catch(() => null) };
}

async function uploadImport(token, projectId, filePath, fileName, options = {}) {
  const fs = await import('fs');
  const fileContent = fs.readFileSync(filePath);
  const blob = new Blob([fileContent]);

  const formData = new FormData();
  formData.append('file', blob, fileName);
  if (options.clean_import) formData.append('clean_import', 'true');
  if (options.skip_auth_schema !== undefined) {
    formData.append('skip_auth_schema', options.skip_auth_schema ? 'true' : 'false');
  }
  if (options.disable_triggers !== undefined) {
    formData.append('disable_triggers', options.disable_triggers ? 'true' : 'false');
  }

  const res = await fetch(`${API}/platform/projects/${projectId}/import`, {
    method: 'POST',
    headers: { 'Authorization': `Bearer ${token}` },
    body: formData,
  });
  return { status: res.status, data: await res.json().catch(() => null) };
}

async function pollUntilDone(token, projectId, taskId, maxWait = 60000) {
  const start = Date.now();
  while (Date.now() - start < maxWait) {
    const { data } = await api('GET', `/platform/projects/${projectId}/import/${taskId}`, null, token);
    if (data && (data.status === 'completed' || data.status === 'failed' || data.status === 'cancelled')) {
      return data;
    }
    await new Promise(r => setTimeout(r, 1000));
  }
  throw new Error('Import timed out');
}

function psql(db, sql) {
  const connStr = `postgresql://${PG_USER}:${PG_PASS}@${PG_HOST}:${PG_PORT}/${db}`;
  return execSync(`psql "${connStr}" -tA -c "${sql}"`, { stdio: 'pipe' }).toString().trim();
}

function psqlFile(db, filePath) {
  const connStr = `postgresql://${PG_USER}:${PG_PASS}@${PG_HOST}:${PG_PORT}/${db}`;
  return execSync(`psql "${connStr}" -f "${filePath}"`, { stdio: 'pipe' }).toString().trim();
}

function pgDump(db, outPath, format = 'custom') {
  const connStr = `postgresql://${PG_USER}:${PG_PASS}@${PG_HOST}:${PG_PORT}/${db}`;
  execSync(`pg_dump --no-owner --no-acl --format=${format} "${connStr}" -f "${outPath}"`, { stdio: 'pipe' });
}

async function run() {
  const tmpDir = '/tmp/import_test_' + Date.now();
  mkdirSync(tmpDir, { recursive: true });

  console.log('\n========================================');
  console.log('  Import Feature E2E Tests');
  console.log('========================================\n');

  // ─── Setup: Create platform user + source project ───
  console.log('--- Setup: Platform user + projects ---');

  const email = `import_test_${Date.now()}@test.com`;
  const password = 'TestPass123!';
  const { data: reg } = await api('POST', '/platform/auth/register', { email, password });
  const token = reg.token;
  assert(!!token, 'Platform registration');

  const { data: srcProj } = await api('POST', '/platform/projects', { name: `src_${Date.now()}` }, token);
  assert(!!srcProj?.id, 'Source project created');

  const { data: dstProj } = await api('POST', '/platform/projects', { name: `dst_${Date.now()}` }, token);
  assert(!!dstProj?.id, 'Destination project created');

  const { data: dstProj2 } = await api('POST', '/platform/projects', { name: `dst2_${Date.now()}` }, token);
  assert(!!dstProj2?.id, 'Second destination project created');

  // ─── Populate source with data ───
  console.log('\n--- Populate source project ---');

  const srcDB = srcProj.db_name;
  const srcSQL = `
    CREATE TABLE public.products (
      id SERIAL PRIMARY KEY,
      name TEXT NOT NULL,
      price NUMERIC(10,2) NOT NULL,
      category TEXT,
      created_at TIMESTAMPTZ DEFAULT NOW()
    );

    CREATE TABLE public.orders (
      id SERIAL PRIMARY KEY,
      product_id INT REFERENCES public.products(id),
      quantity INT NOT NULL DEFAULT 1,
      total NUMERIC(10,2),
      customer_email TEXT,
      created_at TIMESTAMPTZ DEFAULT NOW()
    );

    CREATE TABLE public.reviews (
      id SERIAL PRIMARY KEY,
      product_id INT REFERENCES public.products(id),
      rating INT CHECK (rating >= 1 AND rating <= 5),
      comment TEXT,
      created_at TIMESTAMPTZ DEFAULT NOW()
    );

    INSERT INTO public.products (name, price, category) VALUES
      ('Laptop', 999.99, 'electronics'),
      ('Keyboard', 79.99, 'electronics'),
      ('Desk', 349.99, 'furniture'),
      ('Chair', 449.99, 'furniture'),
      ('Monitor', 599.99, 'electronics');

    INSERT INTO public.orders (product_id, quantity, total, customer_email) VALUES
      (1, 1, 999.99, 'alice@test.com'),
      (2, 2, 159.98, 'bob@test.com'),
      (3, 1, 349.99, 'alice@test.com'),
      (5, 1, 599.99, 'charlie@test.com');

    INSERT INTO public.reviews (product_id, rating, comment) VALUES
      (1, 5, 'Great laptop!'),
      (1, 4, 'Good but expensive'),
      (2, 5, 'Perfect keyboard'),
      (3, 3, 'OK desk'),
      (5, 4, 'Nice monitor');

    GRANT ALL ON ALL TABLES IN SCHEMA public TO anon, authenticated, service_role;
    GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO anon, authenticated, service_role;
  `;

  const srcSQLFile = path.join(tmpDir, 'source_setup.sql');
  writeFileSync(srcSQLFile, srcSQL);
  psqlFile(srcDB, srcSQLFile);

  const productCount = psql(srcDB, "SELECT count(*) FROM public.products");
  assert(productCount === '5', `Source has 5 products (got ${productCount})`);
  const orderCount = psql(srcDB, "SELECT count(*) FROM public.orders");
  assert(orderCount === '4', `Source has 4 orders (got ${orderCount})`);

  // ─── Test 1: Custom format import ───
  console.log('\n--- Test 1: Custom format (pg_dump) import ---');

  const customDumpPath = path.join(tmpDir, 'source.dump');
  pgDump(srcDB, customDumpPath, 'custom');
  assert(existsSync(customDumpPath), 'Custom dump file created');

  const { status: impStatus, data: impTask } = await uploadImport(
    token, dstProj.id, customDumpPath, 'source.dump',
    { skip_auth_schema: true, disable_triggers: true }
  );
  assert(impStatus === 202, `Import started (status ${impStatus})`);
  assert(impTask?.id > 0, `Import task ID returned: ${impTask?.id}`);
  assert(impTask?.status === 'running', `Import status is running`);
  assert(impTask?.format === 'custom', `Format detected as custom`);

  const result1 = await pollUntilDone(token, dstProj.id, impTask.id);
  assert(result1.status === 'completed', `Custom import completed (status: ${result1.status})`);
  assert(result1.tables_imported >= 3, `Tables imported: ${result1.tables_imported} >= 3`);

  // Verify data
  const dstDB = dstProj.db_name;
  const dstProducts = psql(dstDB, "SELECT count(*) FROM public.products");
  assert(dstProducts === '5', `Destination has 5 products (got ${dstProducts})`);
  const dstOrders = psql(dstDB, "SELECT count(*) FROM public.orders");
  assert(dstOrders === '4', `Destination has 4 orders (got ${dstOrders})`);
  const dstReviews = psql(dstDB, "SELECT count(*) FROM public.reviews");
  assert(dstReviews === '5', `Destination has 5 reviews (got ${dstReviews})`);

  // Verify data integrity
  const expensiveProduct = psql(dstDB, "SELECT name FROM public.products WHERE price = 999.99");
  assert(expensiveProduct === 'Laptop', `Expensive product is Laptop (got ${expensiveProduct})`);

  // ─── Test 2: Verify via supabase-js ───
  console.log('\n--- Test 2: Verify imported data via supabase-js ---');

  const svc = createClient(API, dstProj.service_role_key);

  const { data: products, error: prodErr } = await svc.from('products').select('*').order('price', { ascending: false });
  assert(!prodErr, `supabase-js select products: ${prodErr?.message || 'OK'}`);
  assert(products?.length === 5, `5 products via supabase-js`);
  assert(products?.[0]?.name === 'Laptop', `Most expensive is Laptop`);

  const { data: orders } = await svc.from('orders').select('*, products(name)');
  assert(orders?.length === 4, `4 orders with join via supabase-js`);

  const { data: electronicsOrders, error: elErr } = await svc
    .from('orders')
    .select('*, products!inner(name, category)')
    .eq('products.category', 'electronics');
  // This is an advanced join filter - may need PostgREST-level support
  assert(electronicsOrders?.length >= 2 || elErr, `Electronics orders or query attempted: ${electronicsOrders?.length ?? elErr?.message}`);

  // ─── Test 3: Plain SQL import ───
  console.log('\n--- Test 3: Plain SQL import ---');

  const sqlDumpPath = path.join(tmpDir, 'source.sql');
  pgDump(srcDB, sqlDumpPath, 'plain');
  assert(existsSync(sqlDumpPath), 'SQL dump file created');

  const { status: sqlStatus, data: sqlTask } = await uploadImport(
    token, dstProj2.id, sqlDumpPath, 'source.sql',
    { skip_auth_schema: true, disable_triggers: true }
  );
  assert(sqlStatus === 202, `SQL import started (status ${sqlStatus})`);
  assert(sqlTask?.format === 'sql', `Format detected as sql`);

  const result2 = await pollUntilDone(token, dstProj2.id, sqlTask.id);
  assert(result2.status === 'completed', `SQL import completed (status: ${result2.status}, error: ${result2.error_message || 'none'})`);

  // Verify SQL import data
  if (result2.status === 'completed') {
    const dst2DB = dstProj2.db_name;
    const dst2Products = psql(dst2DB, "SELECT count(*) FROM public.products");
    assert(dst2Products === '5', `SQL import: 5 products (got ${dst2Products})`);
  } else {
    assert(false, `SQL import data verification skipped (import failed)`);
  }

  // ─── Test 4: Import history ───
  console.log('\n--- Test 4: Import history ---');

  const { data: history } = await api('GET', `/platform/projects/${dstProj.id}/import/history`, null, token);
  assert(Array.isArray(history), 'Import history is array');
  assert(history.length >= 1, `History has entries: ${history?.length}`);
  assert(history[0].status === 'completed', 'Latest history entry is completed');
  assert(history[0].file_name === 'source.dump', `History file_name matches`);

  // ─── Test 5: Import status check ───
  console.log('\n--- Test 5: Import status ---');

  const { data: statusCheck } = await api('GET', `/platform/projects/${dstProj.id}/import/${impTask.id}`, null, token);
  assert(statusCheck?.status === 'completed', 'Status check returns completed');
  assert(statusCheck?.tables_imported >= 3, `Status check tables_imported: ${statusCheck?.tables_imported}`);
  assert(!!statusCheck?.completed_at, 'Status check has completed_at');

  // ─── Test 6: Error handling - invalid file ───
  console.log('\n--- Test 6: Error handling ---');

  // Use a custom dump format with garbage content to trigger pg_restore failure
  const badFilePath = path.join(tmpDir, 'bad.dump');
  // Write PGDMP magic bytes followed by garbage to trigger custom format detection + pg_restore error
  const badContent = Buffer.from('PGDMP\x00\x00\x00GARBAGE_DATA_NOT_A_REAL_DUMP');
  writeFileSync(badFilePath, badContent);

  const { status: badStatus, data: badTask } = await uploadImport(
    token, dstProj.id, badFilePath, 'bad.dump'
  );
  assert(badStatus === 202, `Bad file import accepted (async)`);

  const badResult = await pollUntilDone(token, dstProj.id, badTask.id);
  assert(badResult.status === 'failed', `Bad file import failed as expected (status: ${badResult.status})`);
  assert(!!badResult.error_message, `Error message provided: ${badResult.error_message?.slice(0, 60)}`);

  // ─── Test 7: Wrong file extension ───
  console.log('\n--- Test 7: File type validation ---');

  const txtFilePath = path.join(tmpDir, 'bad.txt');
  writeFileSync(txtFilePath, 'not a dump');
  const fs = await import('fs');
  const txtContent = fs.readFileSync(txtFilePath);
  const txtBlob = new Blob([txtContent]);
  const txtForm = new FormData();
  txtForm.append('file', txtBlob, 'bad.txt');
  const txtRes = await fetch(`${API}/platform/projects/${dstProj.id}/import`, {
    method: 'POST',
    headers: { 'Authorization': `Bearer ${token}` },
    body: txtForm,
  });
  assert(txtRes.status === 400, `Bad extension rejected (status ${txtRes.status})`);

  // ─── Test 8: Auth check - wrong user can't import ───
  console.log('\n--- Test 8: Auth / ownership checks ---');

  const otherEmail = `other_${Date.now()}@test.com`;
  const { data: otherReg } = await api('POST', '/platform/auth/register', { email: otherEmail, password: 'OtherPass123!' });
  const otherToken = otherReg.token;

  const { status: authStatus } = await uploadImport(
    otherToken, dstProj.id, customDumpPath, 'source.dump'
  );
  assert(authStatus === 404, `Other user cannot import to project (status ${authStatus})`);

  // Other user can't see import history
  const { status: histStatus } = await api('GET', `/platform/projects/${dstProj.id}/import/history`, null, otherToken);
  assert(histStatus === 404, `Other user cannot see import history (status ${histStatus})`);

  // ─── Test 9: Clean import option ───
  console.log('\n--- Test 9: Clean import (drop tables) ---');

  // First add extra data to dst project
  psql(dstDB, "CREATE TABLE IF NOT EXISTS public.temp_table (id SERIAL PRIMARY KEY, val TEXT)");
  psql(dstDB, "INSERT INTO public.temp_table (val) VALUES ('test')");
  const beforeClean = psql(dstDB, "SELECT count(*) FROM pg_tables WHERE schemaname = 'public'");
  assert(parseInt(beforeClean) >= 4, `Before clean: ${beforeClean} tables`);

  const { data: cleanTask } = await uploadImport(
    token, dstProj.id, customDumpPath, 'source.dump',
    { clean_import: true, skip_auth_schema: true, disable_triggers: true }
  );
  const cleanResult = await pollUntilDone(token, dstProj.id, cleanTask.id);
  assert(cleanResult.status === 'completed', `Clean import completed`);

  // temp_table should be gone after clean import
  try {
    psql(dstDB, "SELECT count(*) FROM public.temp_table");
    assert(false, 'temp_table should not exist after clean import');
  } catch {
    assert(true, 'temp_table dropped by clean import');
  }

  // Original data should be restored
  const afterClean = psql(dstDB, "SELECT count(*) FROM public.products");
  assert(afterClean === '5', `After clean import: 5 products`);

  // ─── Summary ───
  console.log('\n========================================');
  console.log(`  Results: ${passed} passed, ${failed} failed (${passed + failed} total)`);
  console.log('========================================\n');

  // Cleanup
  try {
    execSync(`rm -rf ${tmpDir}`, { stdio: 'pipe' });
  } catch {}

  process.exit(failed > 0 ? 1 : 0);
}

run().catch(e => {
  console.error('Test suite error:', e);
  process.exit(1);
});
